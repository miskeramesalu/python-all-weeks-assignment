{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b88a144",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "{\n",
    "\"cells\": [\n",
    "{\n",
    "\"cell_type\": \"markdown\",\n",
    "\"metadata\": {},\n",
    "\"source\": [\n",
    "\"# CORD-19 Analysis Notebook\\n\",\n",
    "\"\\n\",\n",
    "\"This notebook walks through the assignment step-by-step: loading `metadata.csv`, cleaning, analysis, visualizations. Save outputs to the `outputs/` folder.\"\n",
    "]\n",
    "}\n",
    "]\n",
    "},\n",
    "{\n",
    "\"cell_type\": \"code\",\n",
    "\"execution_count\": null,\n",
    "\"metadata\": {},\n",
    "\"outputs\": [],\n",
    "\"source\": [\n",
    "\"# 1. Imports\\n\",\n",
    "\"import pandas as pd\\n\",\n",
    "\"import numpy as np\\n\",\n",
    "\"import matplotlib.pyplot as plt\\n\",\n",
    "\"from collections import Counter\\n\",\n",
    "\"import os\\n\",\n",
    "\"from wordcloud import WordCloud\\n\",\n",
    "\"%matplotlib inline\\n\"\n",
    "]\n",
    "},\n",
    "{\n",
    "\"cell_type\": \"code\",\n",
    "\"execution_count\": null,\n",
    "\"metadata\": {},\n",
    "\"outputs\": [],\n",
    "\"source\": [\n",
    "\"# 2. Load data (adjust path if needed). Use nrows for quick testing.\\n\",\n",
    "\"path = '../data/metadata.csv' # if you run this notebook from notebooks/ folder, adjust path accordingly\\n\",\n",
    "\"# For quick tests, set nrows=50000; set nrows=None to load full file\\n\",\n",
    "\"nrows = None\\n\",\n",
    "\"df = pd.read_csv(path, nrows=nrows, low_memory=False)\\n\",\n",
    "\"print('Loaded rows:', len(df))\\n\",\n",
    "\"df.head()\\n\"\n",
    "]\n",
    "},\n",
    "{\n",
    "\"cell_type\": \"code\",\n",
    "\"execution_count\": null,\n",
    "\"metadata\": {},\n",
    "\"outputs\": [],\n",
    "\"source\": [\n",
    "\"# 3. Basic exploration\\n\",\n",
    "\"print('Shape:', df.shape)\\n\",\n",
    "\"print('\\nColumn dtypes:')\\n\",\n",
    "\"print(df.dtypes)\\n\",\n",
    "\"print('\\nMissing values (top 40):')\\n\",\n",
    "\"print(df.isnull().sum().sort_values(ascending=False).head(40))\\n\"\n",
    "]\n",
    "},\n",
    "{\n",
    "\"cell_type\": \"code\",\n",
    "\"execution_count\": null,\n",
    "\"metadata\": {},\n",
    "\"outputs\": [],\n",
    "\"source\": [\n",
    "\"# 4. Cleaning decisions\\n\",\n",
    "\"# Columns with >80% missing will be dropped by default (change threshold as you like)\\n\",\n",
    "\"missing_frac = df.isnull().mean().sort_values(ascending=False)\\n\",\n",
    "\"drop_cols = missing_frac[missing_frac > 0.8].index.tolist()\\n\",\n",
    "\"print('Dropping columns (>80% missing):', drop_cols)\\n\",\n",
    "\"df_clean = df.drop(columns=drop_cols).copy()\\n\",\n",
    "\"# Drop rows without title (we need titles for text analysis)\\n\",\n",
    "\"df_clean = df_clean.dropna(subset=['title']).copy()\\n\",\n",
    "\"# Parse publish_time\\n\",\n",
    "\"if 'publish_time' in df_clean.columns:\\n\",\n",
    "\" df_clean['publish_time_parsed'] = pd.to_datetime(df_clean['publish_time'], errors='coerce', infer_datetime_format=True)\\n\",\n",
    "\" df_clean['year'] = df_clean['publish_time_parsed'].dt.year\\n\",\n",
    "\"else:\\n\",\n",
    "\" df_clean['year'] = np.nan\\n\",\n",
    "\"# Abstract word count\\n\",\n",
    "\"df_clean['abstract'] = df_clean['abstract'].fillna('')\\n\",\n",
    "\"df_clean['abstract_word_count'] = df_clean['abstract'].str.split().apply(len)\\n\",\n",
    "\"print('After cleaning shape:', df_clean.shape)\\n\",\n",
    "\"df_clean.head()\\n\"\n",
    "]\n",
    "},\n",
    "{\n",
    "\"cell_type\": \"code\",\n",
    "\"execution_count\": null,\n",
    "\"metadata\": {},\n",
    "\"outputs\": [],\n",
    "\"source\": [\n",
    "\"# 5. Analysis: papers by year\\n\",\n",
    "\"os.makedirs('../outputs', exist_ok=True)\\n\",\n",
    "\"if df_clean['year'].notna().any():\\n\",\n",
    "\" year_counts = df_clean['year'].value_counts().sort_index()\\n\",\n",
    "\" print(year_counts.head(20))\\n\",\n",
    "\" # Plot and save\\n\",\n",
    "\" fig, ax = plt.subplots(figsize=(10,4))\\n\",\n",
    "\" year_counts.plot(kind='bar', ax=ax)\\n\",\n",
    "\" ax.set_xlabel('Year'); ax.set_ylabel('Count'); ax.set_title('Publications by Year')\\n\",\n",
    "\" fig.tight_layout()\\n\",\n",
    "\" fig.savefig('../outputs/publications_by_year.png')\\n\",\n",
    "\" display(fig)\\n\",\n",
    "\"else:\\n\",\n",
    "\" print('No publish_time information available to compute year counts.')\\n\"\n",
    "]\n",
    "},\n",
    "{\n",
    "\"cell_type\": \"code\",\n",
    "\"execution_count\": null,\n",
    "\"metadata\": {},\n",
    "\"outputs\": [],\n",
    "\"source\": [\n",
    "\"# Top journals\\n\",\n",
    "\"if 'journal' in df_clean.columns:\\n\",\n",
    "\" top_journals = df_clean['journal'].fillna('Unknown').value_counts().head(30)\\n\",\n",
    "\" print(top_journals.head(20))\\n\",\n",
    "\" fig2, ax2 = plt.subplots(figsize=(8,6))\\n\",\n",
    "\" top_journals.sort_values().plot(kind='barh', ax=ax2)\\n\",\n",
    "\" ax2.set_title('Top 30 journals (paper count)')\\n\",\n",
    "\" fig2.tight_layout()\\n\",\n",
    "\" fig2.savefig('../outputs/top_journals.png')\\n\",\n",
    "\" display(fig2)\\n\",\n",
    "\"else:\\n\",\n",
    "\" print('Column journal not found')\\n\"\n",
    "]\n",
    "},\n",
    "{\n",
    "\"cell_type\": \"code\",\n",
    "\"execution_count\": null,\n",
    "\"metadata\": {},\n",
    "\"outputs\": [],\n",
    "\"source\": [\n",
    "\"# Most frequent words in titles (simple frequency)\\n\",\n",
    "\"titles = df_clean['title'].astype(str).str.lower().str.replace(r'[^a-z0-9\\\\s]', ' ', regex=True)\\n\",\n",
    "\"words = titles.str.split().explode()\\n\",\n",
    "\"stopwords = set(['the','and','of','to','in','a','for','on','with','by','from','is','this','that','covid','covid-19','sars','coronavirus'])\\n\",]\n",
    "\"words = words[~words.isin(stopwords)]}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
